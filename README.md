# üß† ArXiv Abstract Classifier

**ArXiv Abstract Classifier** is an end-to-end Machine Learning web application built with **Streamlit**, designed to automatically **classify scientific paper abstracts** (from arXiv) into major research domains.  

The model leverages **Sentence Embeddings (E5)** and a **LightGBM classifier**, offering both interpretability (via SHAP) and interactivity through an intuitive web interface.

---


## üß© Project Overview

This project extends the research and code in `PaperAbstractModelingSHAP.ipynb` into a full web application.  
It covers the **complete ML workflow**:  
data preprocessing ‚Üí feature engineering ‚Üí model training ‚Üí explainability ‚Üí deployment.

### üéØ Objectives
- Automatically classify arXiv paper abstracts into their respective scientific domains.
- Compare and evaluate various text representation techniques:
  - Bag of Words
  - TF-IDF
  - Sentence Embeddings (E5)
- Experiment with multiple ML algorithms:
  - RandomForest, AdaBoost, GradientBoosting, XGBoost, LightGBM
- Explain model predictions using SHAP values.
- Provide a clean, interactive UI for inference and analysis.

---

## üìö Categories

The classifier predicts one of the following **five scientific fields**:

| Label | Description |
|--------|--------------|
| `astro-ph` | Astrophysics |
| `cond-mat` | Condensed Matter Physics |
| `cs` | Computer Science |
| `math` | Mathematics |
| `physics` | General Physics |

---

## üß† Machine Learning Pipeline

### **1Ô∏è‚É£ Data Preprocessing**
- Dataset: [`UniverseTBD/arxiv-abstracts-large`](https://huggingface.co/datasets/UniverseTBD/arxiv-abstracts-large)
- Filter only abstracts with single labels among the five main domains.
- Text cleaning:
  - Lowercasing, punctuation removal, tokenization
  - Stopword removal using `nltk`

---

### **2Ô∏è‚É£ Feature Representation**
| Method | Description |
|--------|-------------|
| **Bag of Words** | Count vectorization of tokens |
| **TF-IDF** | Weighted term frequency emphasizing unique words |
| **E5 Embeddings** | Sentence-level dense representations using `intfloat/multilingual-e5-base` |

---

### **3Ô∏è‚É£ Models Evaluated**
| Algorithm | Notes |
|------------|-------|
| KMeans / KNN | Baseline clustering & similarity methods |
| Decision Tree / Naive Bayes | Fast interpretable models |
| RandomForest / AdaBoost / GradientBoosting | Ensemble-based strong baselines |
| XGBoost | High-performing gradient boosting |
| **LightGBM** | Final production model due to superior accuracy & inference speed |

---

### **4Ô∏è‚É£ Evaluation Metrics**
- **Accuracy** ‚Äì overall model performance  
- **Precision / Recall / F1-score** ‚Äì per class metrics  
- **Confusion Matrix** ‚Äì class-level error distribution  
- **Coherence Score (LDA)** ‚Äì for topic interpretability  

---

### **5Ô∏è‚É£ Explainability (SHAP)**
Model interpretability powered by **SHAP (SHapley Additive exPlanations)**:
- Global and local feature contributions.
- Visualizations:
  - `summary_plot`
  - `violin_plot`
  - `heatmap`

These explain how features (or words) influenced model predictions.

---

## üåê Web Application

The Streamlit app (`app.py`) provides:
- üßæ **Input**: Abstract text of a scientific paper.
- üéØ **Output**: Predicted research field + confidence probabilities.
- üìä **Extras**:
  - Top similar papers (via cosine similarity on embeddings)
  - Representative keywords per class (via TF-IDF)
  - Visualization of probability distribution

### üñº UI Preview
